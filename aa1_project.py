# -*- coding: utf-8 -*-
"""AA1_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XefibzV_3PdtJj2DFL3dQKk1-XuOVSri

#**GCED - AA1: Pràctica - Breast Cancer Dataset**

El càncer de mama és el càncer més comú entre les dones al món. Representa el 25% de tots els casos de càncer i va afectar més de 2,1 milions de persones només el 2015. Comença quan les cèl·lules de la mama comencen a créixer sense control. Aquestes cèl·lules solen formar tumors que es poden veure mitjançant radiografia o sentir com a bonyes a la zona del pit.


https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic

1. Title: Wisconsin Diagnostic Breast Cancer (WDBC)

2. Source Information

a) Creators:

	Dr. William H. Wolberg, General Surgery Dept., University of
	Wisconsin,  Clinical Sciences Center, Madison, WI 53792
	wolberg@eagle.surgery.wisc.edu

	W. Nick Street, Computer Sciences Dept., University of
	Wisconsin, 1210 West Dayton St., Madison, WI 53706
	street@cs.wisc.edu  608-262-6619

	Olvi L. Mangasarian, Computer Sciences Dept., University of
	Wisconsin, 1210 West Dayton St., Madison, WI 53706
	olvi@cs.wisc.edu

b) Donor: Nick Street

c) Date: November 1995

3. Past Usage:

first usage:

	W.N. Street, W.H. Wolberg and O.L. Mangasarian
	Nuclear feature extraction for breast tumor diagnosis.
	IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science
	and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.

OR literature:

	O.L. Mangasarian, W.N. Street and W.H. Wolberg.
	Breast cancer diagnosis and prognosis via linear programming.
	Operations Research, 43(4), pages 570-577, July-August 1995.

Medical literature:

	W.H. Wolberg, W.N. Street, and O.L. Mangasarian.
	Machine learning techniques to diagnose breast cancer from
	fine-needle aspirates.  
	Cancer Letters 77 (1994) 163-171.

	W.H. Wolberg, W.N. Street, and O.L. Mangasarian.
	Image analysis and machine learning applied to breast cancer
	diagnosis and prognosis.  
	Analytical and Quantitative Cytology and Histology, Vol. 17
	No. 2, pages 77-87, April 1995.

	W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian.
	Computerized breast cancer diagnosis and prognosis from fine
	needle aspirates.  
	Archives of Surgery 1995;130:511-516.

	W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian.
	Computer-derived nuclear features distinguish malignant from
	benign breast cytology.  
	Human Pathology, 26:792--796, 1995.

See also:
	http://www.cs.wisc.edu/~olvi/uwmp/mpml.html
	http://www.cs.wisc.edu/~olvi/uwmp/cancer.html

Results:

	- predicting field 2, diagnosis: B = benign, M = malignant
	- sets are linearly separable using all 30 input features
	- best predictive accuracy obtained using one separating plane
		in the 3-D space of Worst Area, Worst Smoothness and
		Mean Texture.  Estimated accuracy 97.5% using repeated
		10-fold crossvalidations.  Classifier has correctly
		diagnosed 176 consecutive new patients as of November
		1995.

4. Relevant information

	Features are computed from a digitized image of a fine needle
	aspirate (FNA) of a breast mass.  They describe
	characteristics of the cell nuclei present in the image.
	A few of the images can be found at
	http://www.cs.wisc.edu/~street/images/

	Separating plane described above was obtained using
	Multisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree
	Construction Via Linear Programming." Proceedings of the 4th
	Midwest Artificial Intelligence and Cognitive Science Society,
	pp. 97-101, 1992], a classification method which uses linear
	programming to construct a decision tree.  Relevant features
	were selected using an exhaustive search in the space of 1-4
	features and 1-3 separating planes.

	The actual linear program used to obtain the separating plane
	in the 3-dimensional space is that described in:
	[K. P. Bennett and O. L. Mangasarian: "Robust Linear
	Programming Discrimination of Two Linearly Inseparable Sets",
	Optimization Methods and Software 1, 1992, 23-34].


	This database is also available through the UW CS ftp server:

	ftp ftp.cs.wisc.edu
	cd math-prog/cpo-dataset/machine-learn/WDBC/

5. Number of instances: 569

6. Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)

7. Attribute information

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

	a) radius (mean of distances from center to points on the perimeter)
	b) texture (standard deviation of gray-scale values)
	c) perimeter
	d) area
	e) smoothness (local variation in radius lengths)
	f) compactness (perimeter^2 / area - 1.0)
	g) concavity (severity of concave portions of the contour)
	h) concave points (number of concave portions of the contour)
	i) symmetry
	j) fractal dimension ("coastline approximation" - 1)

Several of the papers listed above contain detailed descriptions of
how these features are computed.

The mean, standard error, and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features.  For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.


9. Class distribution: 357 benign, 212 malignan

# Data preprocessing

## Data shape
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn
import pandas as pd
import seaborn as sns
from collections import Counter
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
pd.set_option('display.precision', 3)

from pandas import read_csv
from sklearn.impute import KNNImputer
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor
from sklearn import preprocessing
from pandas.plotting import scatter_matrix
from scipy.stats import boxcox
from statsmodels.genmod.generalized_linear_model import GLM

df = pd.read_csv("breast-cancer-mv.csv")
(df)

#df.describe()
df[['id', 'radius_mean', 'texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean']].describe()

df.iloc[:,[i for i in range(23,33)]].describe()

"""La variable que volem predir és _diagnosis_, que pot prendre valor "M" o "B". Es tracta d'un dataset de classificació."""

df.info()

# drop id , Unnamed: 32 columns
columns_to_drop = ["id", 'Unnamed: 0']

for column in columns_to_drop:
    df.drop(column, axis=1, inplace=True)
df.head()

"""## Variable encoding"""

#encoding de la variable diagnosis
df['diagnosis'] = df['diagnosis'].map({"M":1, "B":0})
df

"""## Spliting train and test"""

X = df.loc[:,df.columns != 'diagnosis']
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

df_train = X_train.copy()
df_train['diagnosis'] = y_train
df_train.head()
X_train.head()

"""## X_train preprocessing

"""

M = B = 0
for e in y_train:
  if e == 1: M += 1
  else: B += 1
print("diagnosis")
print("M", M)
print("B", B)

X_train.shape

sns.countplot(data=df, x=y_train, hue="diagnosis")
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.legend(['benign', 'malignant'])
plt.title('Distribution of Target Values', fontsize=18)
plt.show()

"""### Dealing with X_train missing values"""

column_names = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

for i,column in enumerate(column_names):
    col = i
    plt.subplot(1, 3, i+1)
    sns.histplot(data=X_train[column], bins=10, color='skyblue',kde=True, edgecolor='black')

    # Set labels and title
    # axs[0, col].set_xlabel(f'{column}')
    # axs[0, col].set_ylabel('Frequency')
    # axs[0, col].set_title(f'Histogram of {column}')

# Adjust layout
plt.tight_layout()

(X_train.fractal_dimension_mean==-9999).value_counts()
(X_train.fractal_dimension_se==-9999).value_counts()
(X_train.fractal_dimension_worst==-9999).value_counts()

X_train[X_train.fractal_dimension_mean == -9999][['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']]

"""Podem veure que tenim missing values, indicats amb el valor -9999 a algunes files del dataset a les variables relacionades amb la fractal_dimension (és a dir, les variables fractal_dimension_mean, fractal_dimension_worst i fractal_dimension_se). Canviem els valors d'aquests missing values a NA:"""

X_train.fractal_dimension_mean[df.fractal_dimension_mean == -9999] = np.nan
X_train.fractal_dimension_se[df.fractal_dimension_se == -9999] = np.nan
X_train.fractal_dimension_worst[df.fractal_dimension_worst == -9999] = np.nan


df_train = X_train.copy()
df_train['diagnosis'] = y_train

X_train.describe()[['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']]

X_train.shape
X_train.info()

column_names = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

for i,column in enumerate(column_names):
    col = i
    plt.subplot(1, 3, i+1)
    sns.histplot(data=X_train[column], bins=10, color='skyblue',kde=True, edgecolor='black')

    # Set labels and title
    # axs[0, col].set_xlabel(f'{column}')
    # axs[0, col].set_ylabel('Frequency')
    # axs[0, col].set_title(f'Histogram of {column}')

# Adjust layout
plt.tight_layout()

"""Solucionat!

### Missing values imputation
"""

column_names = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

for i,column in enumerate(column_names):
    col = i
    plt.subplot(1, 3, i+1)
    sns.histplot(data=X_train[column], bins=10, color='skyblue',kde=True, edgecolor='black')

    # Set labels and title
    # axs[0, col].set_xlabel(f'{column}')
    # axs[0, col].set_ylabel('Frequency')
    # axs[0, col].set_title(f'Histogram of {column}')

# Adjust layout
plt.tight_layout()

missing_rows = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']

subset_of_df_train = df_train.drop(columns=missing_rows)
subset_of_df_train.shape

rows_not_missing = df_train.fractal_dimension_mean.notna() & df_train.fractal_dimension_se.notna() & df_train.fractal_dimension_worst.notna()

df_train_without_missing = subset_of_df_train[rows_not_missing]
df_train_without_missing.shape

"""imputation of fractal_dimension_mean"""

fd_mean_missing_values = subset_of_df_train[df_train.fractal_dimension_mean.isna()]
fd_mean_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_train_without_missing, df_train.fractal_dimension_mean[rows_not_missing])
knn_mean = knn.predict(fd_mean_missing_values)

"""imputation of fractal_dimension_se"""

fd_se_missing_values = subset_of_df_train[df_train.fractal_dimension_se.isna()]
fd_se_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_train_without_missing, df_train.fractal_dimension_se[rows_not_missing])
knn_se = knn.predict(fd_se_missing_values)

"""imputation of fractal_dimension_worst"""

fd_worst_missing_values = subset_of_df_train[df_train.fractal_dimension_worst.isna()]
fd_worst_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_train_without_missing, df_train.fractal_dimension_worst[rows_not_missing])
knn_worst = knn.predict(fd_worst_missing_values)

X_train.fractal_dimension_mean[X_train.fractal_dimension_mean.isna()] = knn_mean
X_train.fractal_dimension_se[X_train.fractal_dimension_se.isna()] = knn_se
X_train.fractal_dimension_worst[X_train.fractal_dimension_worst.isna()] = knn_worst

X_train.shape

df_train = X_train.copy()
df_train['diagnosis'] = y_train
df_train.head()

column_names = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

for i,column in enumerate(column_names):
    col = i
    plt.subplot(1, 3, i+1)
    sns.histplot(data=X_train[column], bins=10, color='skyblue',kde=True, edgecolor='black')

    # Set labels and title
    # axs[0, col].set_xlabel(f'{column}')
    # axs[0, col].set_ylabel('Frequency')
    # axs[0, col].set_title(f'Histogram of {column}')

# Adjust layout
plt.tight_layout()

X_train.info()

"""## Dealing with X_test missing values

Ara realitzarem la imputacio dels missing values de X_test sense mirar la distribucio del target ni les variables que imputarem, ja que no volem introduïr cap biaix al nostre model respecte les dades target.

Primer de tot, comprovem on tenim presència de missing values:
"""

(X_test.fractal_dimension_mean==-9999).value_counts()
(X_test.fractal_dimension_se==-9999).value_counts()
(X_test.fractal_dimension_worst==-9999).value_counts()

X_test.fractal_dimension_mean[df.fractal_dimension_mean == -9999] = np.nan
X_test.fractal_dimension_se[df.fractal_dimension_se == -9999] = np.nan
X_test.fractal_dimension_worst[df.fractal_dimension_worst == -9999] = np.nan

df_test = X_test.copy()
df_test['diagnosis'] = y_test

X_test.shape
X_test.info()

missing_rows = ['fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']

subset_of_df_test = df_test.drop(columns=missing_rows)
subset_of_df_test.shape

rows_not_missing = df_test.fractal_dimension_mean.notna() & df_test.fractal_dimension_se.notna() & df_test.fractal_dimension_worst.notna()

df_test_without_missing = subset_of_df_test[rows_not_missing]
df_test_without_missing.shape

fd_mean_missing_values = subset_of_df_test[df_test.fractal_dimension_mean.isna()]
fd_mean_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_test_without_missing, df_test.fractal_dimension_mean[rows_not_missing])
knn_mean = knn.predict(fd_mean_missing_values)

fd_se_missing_values = subset_of_df_test[df_test.fractal_dimension_se.isna()]
fd_se_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_test_without_missing, df_test.fractal_dimension_se[rows_not_missing])
knn_se = knn.predict(fd_se_missing_values)

fd_worst_missing_values = subset_of_df_test[df_test.fractal_dimension_worst.isna()]
fd_worst_missing_values.shape

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(df_test_without_missing, df_test.fractal_dimension_worst[rows_not_missing])
knn_worst = knn.predict(fd_worst_missing_values)

X_test.fractal_dimension_mean[X_test.fractal_dimension_mean.isna()] = knn_mean
X_test.fractal_dimension_se[X_test.fractal_dimension_se.isna()] = knn_se
X_test.fractal_dimension_worst[X_test.fractal_dimension_worst.isna()] = knn_worst
X_test.shape

df_test = X_test.copy()
df_test['diagnosis'] = y_test
df_test.head()



X_test.info()

"""Missing values solucionats!!!

## **OUTLIERS**

Amb aquset dataset haurem d'anar molt en compte a l'hora de treure outliers, ja que si la major part d'outliers pertanyen a cèl·lules malignes, aleshores no els hauríem de treure, ja que aleshores tindriem menys casos malignes en consideració a l'ajustar models.

Exemple radius_mean
"""

df_train.head()

#mirem, per exemple, el cas de la variable radius_mean
fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))
df_train.boxplot(column='radius_mean', ax=axes[0]);
df_train.hist(column='radius_mean', ax=axes[1]);

Q1 = df_train['radius_mean'].quantile(0.25)
Q3 = df_train['radius_mean'].quantile(0.75)
IQR = Q3 - Q1

Q1, Q3, IQR

small_outliers = df_train['radius_mean'] < (Q1 - 1.5 * IQR)
big_outliers = df_train['radius_mean'] > (Q3 + 1.5 * IQR)

print("small outliers:")
print(small_outliers)
print("big outliers:")
print(big_outliers)

df_train.radius_mean.median()

"""Veiem que tenim varis outliers superiors a la mitjana. Aquí hauríem de considerar que tot i ser outliers, potser no ens interessa treurel's del dataset ja que podríen ser casos de cèl·lules cancerígenes.

"""

df_train[small_outliers].diagnosis
print("-----")
df_train[big_outliers].diagnosis

"""En aquest cas, tots els outliers són casos de cèl·lules cancerígenes, i per tant no els hauríem d'excloure del dataset. Per no haver de fer aquest procés per cada variable mirarem les distribucions de probabilitat de cada variable respecte les cèl·lules malignes i benignes."""

data_dia = y_train
data = X_train
data_n_2 = (data - data.mean()) / (data.std())   # standardization
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(14,6))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

data_dia = y_train
data = X_train
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(14,6))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

data_dia = y_train
data = X_train
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(14,6))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

data_dia = y_train
data = X_train
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,20:30]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(14,6))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

"""Conclusions principals: No té gaire sentit considerar la pdf de les variables conjuntament, ja que en general la variància dels tumors malignes es molt més gran i amb una mitjana bastant diferent. Aquesta característica fa que la detecció d'outliers sigui molt més tediosa, ja que variables on el tumor és maligne, tot i que es detectaran com outliers, no es poden treure, ja que perdriem molta informacio."""



# ... more preprocessing to be done



################# end of preprocessing

#SHUFFLE DATA
np.random.seed(144)
data_shuffled = data.sample(frac=1).reset_index(drop=True)

"""# **FEATURE SELECTION**
Amb aquest dataset serà molt important fer una bona features selection, ja que tenim moltes variables i no podrem fer-se servir totes, per tant escollir bé quines tenir en consideració a l'hora d'ajustar models és crucial. Segurament moltes estaràn altament correlades entre elles, ja que moltes variables donen valor a mesures similars, i per suposat, els triplets de variables mean, se i worst probablement també estaràn molt correlades.
"""

f,ax=plt.subplots(figsize = (18,18))
sns.heatmap(df.corr(),annot= True,linewidths=0.5,fmt = ".1f",ax=ax)
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Map')
plt.savefig('graph.png')
plt.show()

"""Efectivament, moltes variables estan altament correlades entre elles. De fet, es poden identificar fins i tot les diagonals (a part, evidenment, de la diagonal princial) que corresponen a les correlacions entre els triplets de variables mencionades, que tendeixen a tenir una alta correlació. Intuïtivament doncs, segurament no podrem utlilitzar m'és d'una variable de les tres.

Per altra banda, s'observa que no hi ha cap variable que estigui negativament correlada a la variable diagnosis (excepte alguns casos, però són correlacions molt baixes).

Ja s'havia comentat a la part d'introducció al dataset que les variables estaven dissenyades tal que més valor de la variable impliqués més probabilitat a ser maligne, i així és compleix.
"""

X = df.drop('diagnosis',axis=1)
y = df['diagnosis']

df = pd.concat([X, y], axis=1)
cm = df.corr()
correlation_with_y = cm.iloc[:-1, -1]

# Visualize the correlation
plt.figure(figsize=(12, 2))
correlation_with_y.sort_values(ascending=False).plot(kind='bar')
plt.title('Correlation of Features with Target Variable (y)')
plt.ylabel('Correlation Coefficient')
plt.show()

""":La major part de les variables estan bastant correlades amb la variable diagnosis, i moltes d'elles ens servirien per ajustar models. Només ens falta com triar les variables (feature selection).

Una manera de fer feature selection és amb _recursive feature elimination_. Consisteix a ajustar un model no gaire complex, com Logistic Regression o Random Forest, i atribuïr pes de significació a les variables, i anar retallant les variables amb menys pes fins arribar a un nombre de variables deitjat (hiperparàmetre a decidir).




Efectivament, moltes variables estan altament correlades entre elles. De fet, es poden identificar fins i tot les diagonals (a part, evidenment, de la diagonal princial) que corresponen a les correlacions entre els triplets de variables mencionades, que tendeixen a tenir una alta correlació. Intuïtivament doncs, segurament no podrem utlilitzar m'és d'una variable de les tres.

Per altra banda, s'observa que no hi ha cap variable que estigui negativament correlada a la variable diagnosis (excepte alguns casos, però són correlacions molt baixes).

Ja s'havia comentat a la part d'introducció al dataset que les variables estaven dissenyades tal que més valor de la variable impliqués més probabilitat a ser maligne, i així és compleix.

Abans de començar amb mètodes de FEATURE SELECTION, podem fer una selecció prèvia de variables que estiguin molt correlacionades entre elles i les que no estàn gens correlacionades amb la variable target, que només són algunes varialbes de se. Abans de res traiem aqusetes variables:

"""

columns_to_drop = ['fractal_dimension_mean', 'texture_se', 'smoothness_se', 'symmetry_se', 'fractal_dimension_se', 'fractal_dimension_worst']
for column in columns_to_drop:
  df.drop(column, axis=1, inplace=True)

"""A continuació mirem quines varialbes estan molt correlacionades entre elles i quedem-nos només amb una d'elles. radius_mean, perimeter_mean, area_mean estan molt correlacionats -> ens quedem amb area_mean."""

columns_to_drop = ['radius_mean', 'area_mean', 'area_se', 'radius_worst', 'perimeter_worst', 'compactness_worst', 'concave.points_worst',
                   'perimeter_se', 'concavity_se']
for column in columns_to_drop:
  df.drop(column, axis=1, inplace=True)

f,ax=plt.subplots(figsize = (18,18))
sns.heatmap(df.corr(),annot= True,linewidths=0.5,fmt = ".1f",ax=ax)
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Map')
plt.savefig('graph.png')
plt.show()

"""
Una alternativa encara més robusta és la _Feature Selection + Cross Validation_, que fa el mateix però ajusta també l'hiperparàmetre del nombre de variables amb les que ens volem quedar. El millor hiperparàmetre es pot decidir en funció de diferents criteris, com _recall_, _accuracy_, _F1 score_...

Utilitzarem l'implementació de Sklearn (RFECV)"""

from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, confusion_matrix
from sklearn.metrics import accuracy_score, recall_score


clf_rf_3 = RandomForestClassifier(random_state=4)
rfecv = RFECV(estimator=clf_rf_3, step=1, cv=5, scoring='recall')   #5-fold cross-validation
rfecv = rfecv.fit(X_train, y_train)


plt.figure(figsize=(10, 4))
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (recall) of number \n of selected features")
plt.plot(range(1, len(rfecv.cv_results_["mean_test_score"]) + 1), rfecv.cv_results_["mean_test_score"])
plt.show()

print('Optimal number of features :', rfecv.n_features_)
print('Best features :', X_train.columns[rfecv.support_])

"""Els pics més alts els tenim a 6, 8 i 11 variables. Per veure quines variables són, repetim l'experiment sense fer CV i fixat el nombre de variables que volguem obtenir."""

from sklearn.feature_selection import RFE

clf_rf = RandomForestClassifier(random_state=43)
rfe = RFE(estimator=clf_rf, n_features_to_select=6, step=1)
rfe = rfe.fit(X_train, y_train)
print('6 best eatures chosen by rfe:',X_train.columns[rfe.support_])

clf_rf = RandomForestClassifier(random_state=43)
rfe = RFE(estimator=clf_rf, n_features_to_select=9, step=1)
rfe = rfe.fit(X_train, y_train)
print('9 best eatures chosen by rfe:',X_train.columns[rfe.support_])

"""# **Modelling -- with all features**"""

def plot_CM(cm, title):
  f, ax = plt.subplots(figsize=(7,5))
  sns.heatmap(cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax)
  plt.title(title)
  plt.xlabel('Y predict')
  plt.ylabel('Y test')
  plt.show()

from sklearn.model_selection import cross_validate

def model_metrics_cv(model, X_train, y_train):
    '''
    X -- Training data
    y -- Traing labels

    returns a dataframe for evaluating metrics (cross-validation)
    '''

    model.fit(X_train, y_train)  #fit the model instance
    cv = cross_validate(model, X_train, y_train, cv=5,
                        scoring=('accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'roc_auc'))


    #extract metrics from cv variable
    accuracy = cv['test_accuracy'].mean()
    balanced_accuracy = cv['test_balanced_accuracy'].mean()
    f1 = cv['test_f1'].mean()
    precision = cv['test_precision'].mean()
    recall = cv['test_recall'].mean()
    roc_auc = cv['test_roc_auc'].mean()

    #create a dataframe to visualize the results
    eval_df = pd.DataFrame([[accuracy, balanced_accuracy, f1, precision, recall, roc_auc]], columns=['accuracy', 'balanced accuracy', 'f1 score', 'precision', 'recall', 'roc auc'])
    return eval_df

from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, balanced_accuracy_score

def model_metrics_test(model, X_train, y_train, X_test,y_test):
    '''
    X_train -- Training data
    y_train -- Traing labels
    X_test  -- Test data
    y_test  -- Test labels

    returns a dataframe for evaluating metrics (test) and a Confusion Matrix
    '''

    model.fit(X_train, y_train)  #fit the model with the whole train set
    test_predictions = model.predict(X_test) # calculate predictions

    # The confusion matrix
    cm = confusion_matrix(y_test, test_predictions)

    #compute metrics for evaluation
    accuracy = accuracy_score(y_test, test_predictions)
    f1 = f1_score(y_test, test_predictions)
    precision = precision_score(y_test, test_predictions)
    recall = recall_score(y_test, test_predictions)
    balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)

    #create a dataframe to visualize the results
    eval_df = pd.DataFrame([[accuracy, balanced_accuracy, f1, precision, recall]], columns=['accuray', 'balanced accuracy', 'f1_score', 'precision', 'recall'])
    return (eval_df, cm)

"""## LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
LDA = LinearDiscriminantAnalysis( )

LDA_result = model_metrics_cv(LDA, X_train, y_train)
LDA_result.index = ["LDA"]
global_results = LDA_result

"""## QDA"""

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
QDA = QuadraticDiscriminantAnalysis()

QDAresult = model_metrics_cv(QDA, X_train, y_train)
QDAresult.index = ["QDA"]
global_results = pd.concat([global_results, QDAresult])

"""## K-nn

Com triem el nombre de veïns?
"""

k_range = range(1, 20)
k_scores = []
from sklearn.model_selection import cross_val_score, KFold
for n in k_range:
  KNN = KNeighborsClassifier(n_neighbors=n)
  score = cross_val_score(KNN, X, y, cv=5, scoring='accuracy')
  k_scores.append(score.mean())


best_k = k_range[np.argmax(k_scores)]
print(f'El millor valor de k és: {best_k}')

# Visualitza els resultats
plt.plot(k_range, k_scores)
plt.xlabel('Valor de k per a KNN')
plt.ylabel('Precisió mitjana amb 10-fold CV')
plt.title('Determinació del millor valor de k per a KNN')
plt.xticks(ticks=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20])
plt.show()

#el mateix però amb scoring='recall'

k_range = range(1, 20)
k_scores = []
from sklearn.model_selection import cross_val_score, KFold
for n in k_range:
  KNN = KNeighborsClassifier(n_neighbors=n)
  score = cross_val_score(KNN, X, y, cv=5, scoring='recall')
  k_scores.append(score.mean())


best_k = k_range[np.argmax(k_scores)]
print(f'El millor valor de k és: {best_k}')

# Visualitza els resultats
plt.plot(k_range, k_scores)
plt.xlabel('Valor de k per a KNN')
plt.ylabel('Precisió mitjana amb 10-fold CV')
plt.title('Determinació del millor valor de k per a KNN')
plt.xticks(ticks=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20])
plt.show()

"""Ens podríem quedar tant amb 3 com amb 5 veïns. Optem per 5."""

KNN = KNeighborsClassifier(n_neighbors=5)
results_knn = model_metrics_cv(KNN, X_train, y_train)
results_knn.index =['KNN']
global_results = pd.concat([global_results, results_knn])

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
Naive_Bayes = GaussianNB()
NB_result= model_metrics_cv(Naive_Bayes, X_train, y_train)
NB_result.index = ['NaiveBayes']
global_results = pd.concat([global_results, NB_result])

"""## Random Forest"""

RFC = RandomForestClassifier()

RFC_result = model_metrics_cv(RFC, X_train, y_train)
RFC_result.index=["Random Forest"]
global_results = pd.concat([global_results, RFC_result])

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()

LogReg_result = model_metrics_cv(lg, X_train, y_train)
LogReg_result.index = ['LogisticReg']
global_results = pd.concat([global_results, LogReg_result])

"""## Neural Network"""

#next year...

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier()
DT_results = model_metrics_cv(DT,X_train, y_train)

DT_results.index = ['DecisionTree']
global_results = pd.concat([global_results, DT_results])

"""## Boosting (AdaBoost)"""

from sklearn.ensemble import AdaBoostClassifier
AdaBoost = AdaBoostClassifier(n_estimators=50, learning_rate=1)

AdaBoost_result = model_metrics_cv(AdaBoost, X_train, y_train)
AdaBoost_result.index = ['AdaBoost']

global_results = pd.concat([global_results, AdaBoost_result])

"""## Boosting (XGBoost)"""

import xgboost as xgb
xgboost = xgb.XGBClassifier()
XGBoost_result = model_metrics_cv(xgboost, X_train, y_train)
XGBoost_result.index = ['XGBoost']

global_results = pd.concat([global_results, XGBoost_result])

import seaborn as sns
tab = global_results.sort_values(by='recall',ascending=False).style.background_gradient(cmap = sns.color_palette("ch:s=-.2,r=.6", as_cmap=True))
tab
print(time.process_time() - start)
#tab.to_html('tabla.html')

"""# **Modelling -- selected features**"""

best_features = ['texture_mean', 'perimeter_mean', 'area_mean', 'concavity_mean',
                 'concave.points_mean', 'area_se', 'radius_worst', 'texture_worst',
                 'perimeter_worst', 'area_worst', 'concave.points_worst']


# Selecciona només les columnes especificades
X_train_selected = X_train[best_features]
X_test_selected = X_test[best_features]

KNN = KNeighborsClassifier(n_neighbors=5, )
results_knn = model_metrics_cv(KNN, X_train_selected, y_train)
results_knn.index =['KNN']
global_results_2 = results_knn


LDA_result = model_metrics_cv(LDA, X_train_selected, y_train)
LDA_result.index = ["LDA"]
global_results_2 = pd.concat([global_results_2, LDA_result])



QDAresult = model_metrics_cv(QDA, X_train_selected, y_train)
QDAresult.index = ["QDA"]
global_results_2 = pd.concat([global_results_2, QDAresult])


Naive_Bayes = GaussianNB()
NB_result = model_metrics_cv(Naive_Bayes, X_train_selected, y_train)
NB_result.index = ['NaiveBayes']
global_results_2 = pd.concat([global_results_2, NB_result])


RFC = RandomForestClassifier()
RFC_result = model_metrics_cv(RFC, X_train_selected, y_train)
RFC_result.index=["Random Forest"]
global_results_2 = pd.concat([global_results_2, RFC_result])


lg = LogisticRegression()
(LogReg_result) = model_metrics_cv(lg, X_train_selected, y_train)
LogReg_result.index = ['LogisticReg']
global_results_2 = pd.concat([global_results_2, LogReg_result])


DT = DecisionTreeClassifier()
DT_results = model_metrics_cv(DT,X_train_selected, y_train)
DT_results.index = ['DecisionTree']
global_results_2 = pd.concat([global_results_2, DT_results])



AdaBoost = AdaBoostClassifier(n_estimators=50, learning_rate=1)
AdaBoost_result = model_metrics_cv(AdaBoost, X_train_selected, y_train)
AdaBoost_result.index = ['AdaBoost']
global_results_2 = pd.concat([global_results_2, AdaBoost_result])


xgboost = xgb.XGBClassifier()
XGBoost_result = model_metrics_cv(xgboost, X_train_selected, y_train)
XGBoost_result.index = ['XGBoost']
global_results_2 = pd.concat([global_results_2, XGBoost_result])


#plot results
global_results_2.sort_values(by='recall',ascending=False).style.background_gradient(cmap = sns.color_palette("ch:s=-.2,r=.6", as_cmap=True))

"""#**Modelling with standarization**"""

X_train_stand = (X_train - X_train.mean()) / X_train.std()
X_test_stand = (X_test - X_test.mean()) / X_test.std()



best_features = ['texture_mean', 'perimeter_mean', 'area_mean', 'concavity_mean',
                 'concave.points_mean', 'area_se', 'radius_worst', 'texture_worst',
                 'perimeter_worst', 'area_worst', 'concave.points_worst']




# Selecciona només les columnes especificades
X_train_stand_selected = X_train_stand[best_features]
X_test_stand_selected = X_test_stand[best_features]

import time
start = time.process_time()
# your code here




KNN = KNeighborsClassifier(n_neighbors=5)
results_knn = model_metrics_cv(KNN, X_train_stand_selected, y_train)
results_knn.index =['KNN']
global_results_2 = results_knn


LDA_result = model_metrics_cv(LDA, X_train_stand_selected, y_train)
LDA_result.index = ["LDA"]
global_results_2 = pd.concat([global_results_2, LDA_result])



QDAresult = model_metrics_cv(QDA, X_train_stand_selected, y_train)
QDAresult.index = ["QDA"]
global_results_2 = pd.concat([global_results_2, QDAresult])


Naive_Bayes = GaussianNB()
NB_result = model_metrics_cv(Naive_Bayes, X_train_stand_selected, y_train)
NB_result.index = ['NaiveBayes']
global_results_2 = pd.concat([global_results_2, NB_result])



RFC = RandomForestClassifier()
RFC_result = model_metrics_cv(RFC, X_train_stand_selected, y_train)
RFC_result.index=["Random Forest"]
global_results_2 = pd.concat([global_results_2, RFC_result])


lg = LogisticRegression()
(LogReg_result) = model_metrics_cv(lg, X_train_stand_selected, y_train)
LogReg_result.index = ['LogisticReg']
global_results_2 = pd.concat([global_results_2, LogReg_result])


DT = DecisionTreeClassifier()
DT_results = model_metrics_cv(DT,X_train_stand_selected, y_train)
DT_results.index = ['DecisionTree']
global_results_2 = pd.concat([global_results_2, DT_results])



AdaBoost = AdaBoostClassifier(n_estimators=50, learning_rate=1)
AdaBoost_result = model_metrics_cv(AdaBoost, X_train_stand_selected, y_train)
AdaBoost_result.index = ['AdaBoost']
global_results_2 = pd.concat([global_results_2, AdaBoost_result])


xgboost = xgb.XGBClassifier()
XGBoost_result = model_metrics_cv(xgboost, X_train_stand_selected, y_train)
XGBoost_result.index = ['XGBoost']
global_results_2 = pd.concat([global_results_2, XGBoost_result])



print(time.process_time() - start)

global_results_2.sort_values(by='recall',ascending=False).style.background_gradient(cmap = sns.color_palette("ch:s=-.2,r=.6", as_cmap=True))

"""# **Final models**"""

QDAresult, cmQDA = model_metrics_test(QDA, X_train, y_train, X_test, y_test)
QDAresult.index = ["QDA"]
final_result = QDAresult
plot_CM(cmQDA, "QDA Classifier Confusion Matrix")


AdaBoost_result, cmAB = model_metrics_test(AdaBoost, X_train_selected, y_train, X_test_selected, y_test)
AdaBoost_result.index = ['AdaBoost']
final_result = pd.concat([final_result, AdaBoost_result])
plot_CM(cmAB, "AdaBoost Classifier Confusion Matrix")



(LogReg_result, cmLR) = model_metrics_test(lg, X_train_stand_selected, y_train, X_test_stand_selected, y_test)
LogReg_result.index = ['LogisticReg']
final_result = pd.concat([LogReg_result, final_result])
plot_CM(cmLR, "Logistic Regression Classifier Confusin Matrix")

final_result.sort_values(by='recall',ascending=False).style.background_gradient(cmap = sns.color_palette("ch:s=-.2,r=.6", as_cmap=True))